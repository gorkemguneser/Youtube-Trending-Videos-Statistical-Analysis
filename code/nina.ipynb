{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Trending Videos Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to be edited</b>\n",
    "<b>https://www.kaggle.com/rsrishav/youtube-trending-video-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import t\n",
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Later on we can also remove the ones we wouldn't use\n",
    "'''\n",
    "# Importing dataset\n",
    "data_gb = pd.read_csv(\"../data/GB_youtube_trending_data.csv\",sep=\",\")  #uk  \n",
    "data_fr = pd.read_csv(\"../data/FR_youtube_trending_data.csv\",sep=\",\")  #france\n",
    "data_us = pd.read_csv(\"../data/US_youtube_trending_data.csv\",sep=\",\")  #usa \n",
    "data_ru = pd.read_csv(\"../data/RU_youtube_trending_data.csv\",sep=\",\")  #russia\n",
    "data_de = pd.read_csv(\"../data/DE_youtube_trending_data.csv\",sep=\",\")  #germany \n",
    "data_ca = pd.read_csv(\"../data/CA_youtube_trending_data.csv\",sep=\",\")  #canada   \n",
    "data_kr = pd.read_csv(\"../data/KR_youtube_trending_data.csv\",sep=\",\")  #southkorea   \n",
    "data_jp = pd.read_csv(\"../data/JP_youtube_trending_data.csv\",sep=\",\")  #japan\n",
    "data_br = pd.read_csv(\"../data/BR_youtube_trending_data.csv\",sep=\",\")  #brazil\n",
    "data_mx = pd.read_csv(\"../data/MX_youtube_trending_data.csv\",sep=\",\")  #mexico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Property</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>video_id</td>\n",
       "      <td>J78aPJ3VyNs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>title</td>\n",
       "      <td>I left youtube for a month and THIS is what ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>publishedAt</td>\n",
       "      <td>2020-08-11T16:34:06Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>channelId</td>\n",
       "      <td>UCYzPXprvl5Y-Sf0g4vX-m6g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>channelTitle</td>\n",
       "      <td>jacksepticeye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>categoryId</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>trending_date</td>\n",
       "      <td>2020-08-12T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tags</td>\n",
       "      <td>jacksepticeye|funny|funny meme|memes|jacksepti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>view_count</td>\n",
       "      <td>2038853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>likes</td>\n",
       "      <td>353790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dislikes</td>\n",
       "      <td>2628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>comment_count</td>\n",
       "      <td>40228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>thumbnail_link</td>\n",
       "      <td>https://i.ytimg.com/vi/J78aPJ3VyNs/default.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>comments_disabled</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ratings_disabled</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>description</td>\n",
       "      <td>I left youtube for a month and this is what ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Property                                             Output\n",
       "0            video_id                                        J78aPJ3VyNs\n",
       "1               title  I left youtube for a month and THIS is what ha...\n",
       "2         publishedAt                               2020-08-11T16:34:06Z\n",
       "3           channelId                           UCYzPXprvl5Y-Sf0g4vX-m6g\n",
       "4        channelTitle                                      jacksepticeye\n",
       "5          categoryId                                                 24\n",
       "6       trending_date                               2020-08-12T00:00:00Z\n",
       "7                tags  jacksepticeye|funny|funny meme|memes|jacksepti...\n",
       "8          view_count                                            2038853\n",
       "9               likes                                             353790\n",
       "10           dislikes                                               2628\n",
       "11      comment_count                                              40228\n",
       "12     thumbnail_link     https://i.ytimg.com/vi/J78aPJ3VyNs/default.jpg\n",
       "13  comments_disabled                                              False\n",
       "14   ratings_disabled                                              False\n",
       "15        description  I left youtube for a month and this is what ha..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This can stay for now to easily see what are our properties in the dataset. We can delete it once\n",
    "we are familiar with the dataset\n",
    "'''\n",
    "brief_list_col = []\n",
    "brief_list = []\n",
    "\n",
    "for i in range(0,len(data_gb.columns)):\n",
    "    brief_list_col.append(data_gb.columns[i])\n",
    "    brief_list.append(data_gb[data_gb.columns[i]][0])\n",
    "\n",
    "brief_df = pd.DataFrame(brief_list_col,columns=[\"Property\"])\n",
    "brief_df.insert(1,\"Output\",brief_list,True)\n",
    "brief_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adding json files\n",
    "'''\n",
    "gb_json = pd.read_json('../data/GB_category_id.json')\n",
    "fr_json = pd.read_json('../data/FR_category_id.json')\n",
    "us_json = pd.read_json('../data/US_category_id.json')\n",
    "ru_json = pd.read_json('../data/RU_category_id.json')\n",
    "de_json = pd.read_json('../data/DE_category_id.json')\n",
    "ca_json = pd.read_json('../data/CA_category_id.json')\n",
    "kr_json = pd.read_json('../data/KR_category_id.json')\n",
    "jp_json = pd.read_json('../data/JP_category_id.json')\n",
    "br_json = pd.read_json('../data/BR_category_id.json')\n",
    "mx_json = pd.read_json('../data/MX_category_id.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defining function to flat json files to get rid of dictionary format for every country\n",
    "'''\n",
    "\n",
    "col_list = [\"items\",\"items_snippet\"]\n",
    "\n",
    "def flat_func(df,column):\n",
    "    for column in col_list:\n",
    "        flatten = pd.DataFrame(dict(df[column])).transpose()\n",
    "        columns = [str(i) for i in flatten.columns]\n",
    "        flatten.columns = [column + \"_\" + str(colname) for colname in columns]\n",
    "        df = pd.concat([df,flatten],axis=1)\n",
    "        df = df.drop(column,axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Applying function\n",
    "Creating flat databases\n",
    "'''\n",
    "\n",
    "gb_flat = flat_func(gb_json,col_list)[[\"items_id\",\"items_snippet_title\"]]\n",
    "fr_flat = flat_func(fr_json,col_list)[[\"items_id\",\"items_snippet_title\"]]\n",
    "us_flat = flat_func(us_json,col_list)[[\"items_id\",\"items_snippet_title\"]]\n",
    "ru_flat = flat_func(ru_json,col_list)[[\"items_id\",\"items_snippet_title\"]]\n",
    "de_flat = flat_func(de_json,col_list)[[\"items_id\",\"items_snippet_title\"]]\n",
    "ca_flat = flat_func(ca_json,col_list)[[\"items_id\",\"items_snippet_title\"]]\n",
    "kr_flat = flat_func(kr_json,col_list)[[\"items_id\",\"items_snippet_title\"]]\n",
    "jp_flat = flat_func(jp_json,col_list)[[\"items_id\",\"items_snippet_title\"]]\n",
    "br_flat = flat_func(br_json,col_list)[[\"items_id\",\"items_snippet_title\"]]\n",
    "mx_flat = flat_func(mx_json,col_list)[[\"items_id\",\"items_snippet_title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dropping categoryId 29 for each country\n",
    "'''\n",
    "\n",
    "data_countries = [data_gb,data_fr,data_us,data_ru,data_de,data_ca,data_kr,data_jp,data_br,data_mx]\n",
    "\n",
    "def category_remover(country):\n",
    "    drop_idx = country[country[\"categoryId\"]==29].index\n",
    "    return drop_idx\n",
    "\n",
    "data_gb = data_gb.drop(category_remover(data_gb))\n",
    "data_fr = data_fr.drop(category_remover(data_fr))\n",
    "data_us = data_us.drop(category_remover(data_us))\n",
    "data_ru = data_ru.drop(category_remover(data_ru))\n",
    "data_de = data_de.drop(category_remover(data_de))\n",
    "data_ca = data_ca.drop(category_remover(data_ca))\n",
    "data_kr = data_kr.drop(category_remover(data_kr))\n",
    "data_jp = data_jp.drop(category_remover(data_jp))\n",
    "data_br = data_br.drop(category_remover(data_br))\n",
    "data_mx = data_mx.drop(category_remover(data_mx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating function to add trend name next to categoryId for every country\n",
    "'''\n",
    "def trend_adder(country_df,country_flat):\n",
    "    trend = []\n",
    "    for trend_id in country_df[\"categoryId\"]:\n",
    "        for i in range(0,len(country_flat)):\n",
    "            if trend_id == int(country_flat[\"items_id\"][i]):\n",
    "                trend.append(country_flat[\"items_snippet_title\"][i])\n",
    "\n",
    "    country_df.insert(6,\"trend\",trend)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Applying Function\n",
    "Adding trend column next to categoryId for every country\n",
    "'''\n",
    "trend_order_data_country = [data_gb,data_fr,data_us,data_ru,data_de,\n",
    "                            data_ca,data_kr,data_jp,data_br,data_mx]\n",
    "\n",
    "trend_order_data_flat = [gb_flat,fr_flat,us_flat,ru_flat,de_flat,\n",
    "                         ca_flat,kr_flat,jp_flat,br_flat,mx_flat]\n",
    "\n",
    "for i in range(0,len(trend_order_data_country)):\n",
    "    trend_adder(trend_order_data_country[i],trend_order_data_flat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scrapping the most updated row for each link.\n",
    "\n",
    "Not the best way but kinda stuck while trying to retrieve local variables from function so this will\n",
    "give us what we want for now..\n",
    "'''\n",
    "def get_recent_idx(country):\n",
    "    latest_idx = []\n",
    "    for link in np.array(country[\"video_id\"].unique()):\n",
    "        latest_row = country[(country[\"video_id\"] ==\n",
    "                                      link)][[\"view_count\"]].nlargest(1,\"view_count\").index[0]\n",
    "    \n",
    "        latest_idx.append(latest_row)\n",
    "    return country[country.index.isin(latest_idx)]\n",
    "   \n",
    "data_gb_unique = get_recent_idx(data_gb)\n",
    "data_fr_unique = get_recent_idx(data_fr)\n",
    "data_us_unique = get_recent_idx(data_us)\n",
    "data_ru_unique = get_recent_idx(data_ru)\n",
    "data_de_unique = get_recent_idx(data_de)\n",
    "data_ca_unique = get_recent_idx(data_ca)\n",
    "data_kr_unique = get_recent_idx(data_kr)\n",
    "data_jp_unique = get_recent_idx(data_jp)\n",
    "data_br_unique = get_recent_idx(data_br)\n",
    "data_mx_unique = get_recent_idx(data_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['video_id', 'title', 'publishedAt', 'channelId', 'channelTitle',\n",
       "       'categoryId', 'trend', 'trending_date', 'tags', 'view_count', 'likes',\n",
       "       'dislikes', 'comment_count', 'thumbnail_link', 'comments_disabled',\n",
       "       'ratings_disabled', 'description'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gb_unique.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis \n",
    "\n",
    "View counts are largely similar for the US and UK\n",
    "\n",
    "H0: mu UK == mu US\n",
    "\n",
    "H1: mu UK != mu US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2128238.6995253745"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gb_unique['view_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7214438.945745977"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gb_unique['view_count'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3041465.1761052245"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_us_unique['view_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9549893.37796264"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_us_unique['view_count'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=3.9923474414589903, pvalue=6.634332324974839e-05)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "ttest_ind(data_us_unique['view_count'], data_gb_unique['view_count'], equal_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P value is much smaller than critical alpha value (0.05) therefore we can reject the null hypothesis and assume that the average number of views for a video in the US is statistically different from in the UK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis \n",
    "\n",
    "The average number of likes on a trending video is greater than 200k \n",
    "\n",
    "H0: mean == 200k\n",
    "H1: mean >200k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "us_likes = data_us_unique['likes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156722.9039935458"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_likes.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552001.2496020874"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_likes.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 200000\n",
    "x_bar = us_likes.mean()\n",
    "sigma = us_likes.std()\n",
    "marg_error = sigma / len(us_likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-194.35448937359482\n"
     ]
    }
   ],
   "source": [
    "z = (x_bar - mu) / marg_error\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "p = norm.cdf(z)\n",
    "print(1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alternate test\n",
    "p_values = scipy.stats.norm.sf(abs(z))\n",
    "p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a small p value meens null hypothesis can be rejected and therefore we can assume mean is greater than 200k for likes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 95% confidence, the true mean lies in the inverval: [134982.77747488924, 178463.03051220236]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "us_likes\n",
    "us_likes_stdev = us_likes.std()\n",
    "degree_freedom = len(us_likes) - 1\n",
    "t = stats.t.interval(0.95, degree_freedom)[1]\n",
    "\n",
    "# Terms\n",
    "mean = us_likes.mean()\n",
    "std_sample = np.sqrt(abs(np.sum((us_likes - mean)**2) / (1-len(us_likes))))\n",
    "marg_of_error = std_sample / np.sqrt(len(us_likes))\n",
    "con_int = [mean - (t * marg_of_error) , mean + (t * marg_of_error)]\n",
    "\n",
    "print('With 95% confidence, the true mean lies in the inverval:', con_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible hypothesis;\n",
    "\n",
    "more popular the video more like/less dislike,\n",
    "tbc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
